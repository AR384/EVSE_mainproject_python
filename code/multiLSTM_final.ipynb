{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23573b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,RobustScaler\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score ,classification_report,confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modelzoo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9a383",
   "metadata": {},
   "source": [
    "1. 전처리된 df → 시계열 형태로 정렬\n",
    "2. Sliding Window 등으로 (samples, timesteps, features) 구성\n",
    "3. MinMaxScaler로 시퀀스 스케일링\n",
    "4. LSTM 모델 설계 (Keras 기반 추천)\n",
    "5. 학습 및 예측\n",
    "6. 분류/회귀 성능 비교\n",
    "7. 베이스라인보다 나은지 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계 전처리 충전소별로 시간순 정렬\n",
    "df = pd.read_csv('../data/csv/50area_dummy_processed.csv')\n",
    "df_sort = df.sort_values(by=['station_location','connection_start_time_ts']).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10439214",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 'post_charge_departure_range'\n",
    "target_reg = 'kwh_per_usage_time'\n",
    "# sort colums\n",
    "onehot_col = ['station_location','evse_name','evse_type','supports_discharge','scheduled_charge','weekday','cluster','post_charge_departure_range','usage_departure_range'] \n",
    "scale_col = [col for col in df.columns.to_list() if col not in onehot_col]\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        ('scaling',RobustScaler(),scale_col),\n",
    "        ('onehot',OneHotEncoder(sparse_output=False,handle_unknown='ignore'),onehot_col)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#지점별로 시퀀스 분리\n",
    "seq_len = 24 #48시간 동안의 특징 파악 \n",
    "max_n = 1*24 # 최대 예측 n (ex 1주일)\n",
    "\n",
    "# 슬라이딩 윈도우 함수\n",
    "def create_sequences(X_df, y_class_series, y_reg_series, feature_cols, seq_len, max_n):\n",
    "    X_seq, y_class, y_reg = [], [], []\n",
    "\n",
    "    for i in range(len(X_df) - seq_len - max_n + 1):\n",
    "        X_seq.append(X_df[feature_cols].iloc[i:i+seq_len].values)\n",
    "        y_class.append(y_class_series.iloc[i + seq_len : i + seq_len + max_n].values)\n",
    "        y_reg.append(y_reg_series.iloc[i + seq_len : i + seq_len + max_n].values)\n",
    "\n",
    "    return np.array(X_seq), np.array(y_class), np.array(y_reg)\n",
    "\n",
    "#지점별 시간대 정렬후 데이터 합치기\n",
    "def create_data(ct,df_sort,max_n,target_class,target_reg,seq_len):\n",
    "    all_X, all_y_class, all_y_reg = [], [], []\n",
    "    \n",
    "    ct.fit(df_sort)\n",
    "    \n",
    "    for _, group in df_sort.groupby('station_location'):\n",
    "        if len(group) < seq_len+max_n:\n",
    "            continue\n",
    "        group_sorted = group.sort_values('charging_start_time_ts')\n",
    "        \n",
    "        # ColumnTransformer 적용\n",
    "        X_trans = ct.transform(group_sorted)\n",
    "        df_trans = pd.DataFrame(X_trans, columns=ct.get_feature_names_out())\n",
    "        # 원본에서 타겟 시리즈 추출\n",
    "        y_class_series = group_sorted[target_class]\n",
    "        y_reg_series = group_sorted[target_reg]\n",
    "\n",
    "        # 슬라이딩 윈도우 생성\n",
    "        X_seq, y_class, y_reg = create_sequences(\n",
    "            df_trans,\n",
    "            y_class_series,\n",
    "            y_reg_series,\n",
    "            feature_cols=ct.get_feature_names_out(),\n",
    "            seq_len=seq_len,\n",
    "            max_n=max_n\n",
    "        )\n",
    "        all_X.append(X_seq)\n",
    "        all_y_class.append(y_class)\n",
    "        all_y_reg.append(y_reg)\n",
    "\n",
    "    x_all = np.concatenate(all_X,axis=0)\n",
    "    y_class_all = np.concatenate(all_y_class , axis=0)\n",
    "    y_reg_all = np.concatenate(all_y_reg, axis=0)\n",
    "    \n",
    "    return x_all, y_class_all, y_reg_all\n",
    "\n",
    "x_all, y_class_all, y_reg_all = create_data(ct,df_sort,max_n,target_class,target_reg,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c910d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3단계: LSTM 멀티태스크 모델 구성 (회귀 + 분류)\n",
    "#모델1\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight  # 클래스 가중치 (불균형 대응)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = F.log_softmax(input, dim=1)\n",
    "        p = torch.exp(logp)\n",
    "        logp = (1 - p) ** self.gamma * logp\n",
    "        loss = F.nll_loss(logp, target, weight=self.weight)\n",
    "        return loss \n",
    "\n",
    "class MultiTargetDataset(Dataset):\n",
    "    def __init__(self,x,n,y_regs,y_clss):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.n = torch.tensor(n,dtype=torch.float32)\n",
    "        self.y_reg = torch.tensor(y_regs,dtype=torch.float32)\n",
    "        self.y_cls =torch.tensor(y_clss,dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx],self.n[idx],self.y_reg[idx],self.y_cls[idx]\n",
    "\n",
    "#data\n",
    "X,n_array,y_regs,y_clss = create_seq(df,data_scaled,seq_len,max_n)\n",
    "\n",
    "#hyper parameter\n",
    "input_dim = X.shape[2]\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "num_classes = len(np.unique(y_clss))\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = LSTMwithMultiOutput(input_dim,hidden_dim,num_layers,num_classes).to(device)\n",
    "# model = BiLSTMwithMultiOutput(input_dim,hidden_dim,num_layers,num_classes).to(device)\n",
    "dataset = MultiTargetDataset(X,n_array,y_regs,y_clss)\n",
    "loader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "unique_classes = np.unique(y_clss)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_clss)\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "loss_fn_reg = nn.MSELoss()\n",
    "# loss_fn_cls = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "loss_fn_cls = FocalLoss(gamma=2.0,weight=weights_tensor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(y_clss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b64978",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_reg_loss, total_cls_loss = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for xb, nb, yb_reg, yb_cls in loader:\n",
    "        xb, nb = xb.to(device), nb.to(device)\n",
    "        yb_reg, yb_cls = yb_reg.to(device), yb_cls.to(device)\n",
    "\n",
    "        pred_reg, pred_cls = model(xb, nb)  # 회귀, 분류 예측값\n",
    "        # print(yb_reg.min().item(), yb_reg.max().item())  # 정규화된 범위인지\n",
    "        # print(pred_reg.min().item(), pred_reg.max().item())  # 튀는 값 있는지\n",
    "        # print(pred_reg.shape, yb_reg.shape)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss_reg = loss_fn_reg(pred_reg, yb_reg)\n",
    "        loss_cls = loss_fn_cls(pred_cls, yb_cls)\n",
    "        \n",
    "        alpha = 1.0  # 회귀 손실 비중\n",
    "        beta = 2.0   # 분류 손실 비중 (더 작게 시작)\n",
    "\n",
    "        loss = alpha * loss_reg + beta * loss_cls\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reg_loss += loss_reg.item()\n",
    "        total_cls_loss += loss_cls.item()\n",
    "\n",
    "        # 분류 정확도 계산\n",
    "        all_preds.extend(pred_cls.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(yb_cls.cpu().numpy())\n",
    "        \n",
    "\n",
    "    n_batches = len(loader)\n",
    "        \n",
    "    scheduler.step()\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    avg_loss = total_loss / n_batches\n",
    "    avg_reg  = total_reg_loss  / n_batches\n",
    "    avg_cls  = total_cls_loss  / n_batches\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')       # 다중 클래스용\n",
    "    precision = precision_score(all_labels, all_preds, average='macro',zero_division=1)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"[{epoch+1}] LR = {curr_lr:.5f} Loss: {avg_loss:.4f} | Reg: {avg_reg:.4f} | Cls: {avg_cls:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | precision: {precision:.4f} | recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd401b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 → 정수 클래스 → 문자열로 디코딩\n",
    "model.eval()  # <--- Add this line to set model to eval mode\n",
    "with torch.no_grad():\n",
    "    x_sample = X[0]                # shape: (seq_len, input_dim)\n",
    "    n_sample = n_array[0]\n",
    "    x_tensor = torch.tensor(x_sample, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    n_tensor = torch.tensor(n_sample, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    pred_energy, pred_state = model(x_tensor, n_tensor)\n",
    "    pred_state_label = target_cls_le.inverse_transform([int(pred_state.argmax(1).item())])[0]\n",
    "    real_state_label = target_cls_le.inverse_transform([int(y_clss[0])])[0]\n",
    "    pred_energy_cpu = pred_energy.detach().cpu().numpy().reshape(-1, 1)\n",
    "    pred_energy_inv = scaler_y.inverse_transform(pred_energy_cpu)\n",
    "print(f\"예측 장비 상태: {pred_state_label}, 실제 상태: {real_state_label}\")\n",
    "print(f\"예측 에너지 사용량 (역정규화값): {pred_energy_inv.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b86a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "# torch.save(model,'../model/bilstm_0.48.pt')\n",
    "bilstms = torch.load('../model/bilstm_0.48.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력 n 시간뒤 예측\n",
    "sample_seq = X[10000]\n",
    "sample_seq_tensor = torch.tensor(sample_seq,dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "def predict_n_hours_ahead(n_hour, model):\n",
    "    n_norm = n_hour / max_n\n",
    "    n_tensor = torch.tensor([[n_norm]], dtype=torch.float32).to(device)  # shape (1, 1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred_reg, pred_cls = model(sample_seq_tensor, n_tensor)\n",
    "        pred_energy = scaler_y.inverse_transform(pred_reg.cpu().numpy().reshape(-1, 1))[0][0]\n",
    "        pred_cls_label = target_cls_le.inverse_transform([torch.argmax(pred_cls, dim=1).item()])[0]\n",
    "    return pred_energy, pred_cls_label\n",
    "\n",
    "n = 3 # 12시간 뒤 예측\n",
    "에너지, 상태 = predict_n_hours_ahead(n,bilstms)\n",
    "print(f\"{n}시간 뒤 에너지 사용량: {에너지:.2f}, 장비 상태: {상태}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e02491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_cls_le.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b189bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class별 F1:\", f1_score(all_labels, all_preds, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_clss, bins=np.arange(num_classes+1)-0.5, rwidth=0.8)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(range(num_classes))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloserver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
